---
title: "Class 14 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$





```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)


options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview

 * Kruskal–Wallis test and pairwise comparisons
 * Simple linear regression



$\\$





**Part 1.0 - step 0** Create a side-by-side boxplot comparing the critics' scores of the movie for each MPAA rating level. 

```{r visualize_movies}


#download.file('http://www2.stat.duke.edu/~mc301/data/movies.Rdata', 'movies.Rdata')

load('movies.Rdata')

# only keep movies rated "G", "PG", "PG-13", "R"
movies3 <- movies[movies$mpaa_rating %in% c("G", "PG", "PG-13", "R"), ]
movies3$mpaa_rating <- droplevels(movies3$mpaa_rating)



boxplot(critics_score ~ mpaa_rating, data = movies3, 
        xlab = "MPAA rating", 
        ylab = "Critics' score",
        main = "Critics' scores for each MPAA rating level")
```




$\\$


**Part 1.1** Kruskal–Wallis test to see if any of our groups stochastically dominate another group. 

```{r}

#Kruskal–Wallis test
kruskal.test(critics_score ~ mpaa_rating, data = movies3)


# compare to the ANOVA 
anova_fit <- aov(critics_score ~ mpaa_rating, data = movies3)
summary.aov(anova_fit)



```


Small p-value so we have domination!




$\\$




**Part 1.2** Pairwise comparisons in R


```{r}


# test with no multiple comparisons adjustment (not good)
(pair_tests <- pairwise.t.test(movies3$critics_score, movies3$mpaa_rating, p.adj = "none"))


# with the Bonferroni
pairwise.t.test(movies3$critics_score, movies3$mpaa_rating, p.adj = "bonferroni")


# Note, the Bonferroni p-values are 6 times larger than the p-values with no adjustment
6 * pair_tests$p.value


# Tukey's HSD test using the TukeyHSD(anova_fit) function - very similar to the Bonferroni correction here
TukeyHSD(anova_fit)



```





      G      PG     PG-13 
PG    1.0000 -      -     
PG-13 0.0375 0.0407 -     
R     1.0000 1.0000 0.0019

P value adjustment method: bonferroni 










$\\$


## Part 2: Simple linear regression in R



Smoking is associated with cancer? Let's do a regression analysis to build a linear model that can predict the cancer rate based on the rate that people smoke using the smoking rate from 50 states. 



Let's start by plotting the data, fitting our linear model, and visualizing the fit


```{r}

# source('class_functions.R')
# download_class_data("smoking_cancer.rda")


load("smoking_cancer.rda")


# create a scatter plot and calculate the correlation (note: this is plot(x, y))
plot(smoking$CIG, smoking$LUNG,
     xlab = "Cigarettes sold per capita",
     ylab = "Lung cancer deaths per 100,000") 

# fit a regression model  (note: this is y as as function of x)
lm_fit <- lm(LUNG ~ CIG, data = smoking) 


# add the regression line to the plot
abline(lm_fit)


# do any points appear to be outliers? 



```





$\\$




Now let's examine:

1) The coeffients found by our model
2) The predicted values y-hat values for each x value in our data set
3) The residuals



```{r}


# examine the a and b coefficients
coef(lm_fit)


# could you write out this linear regression equation? 



# let's look at all the fitted values y-hat for each x in our data set
lm_fit$fitted.values


# let's look at the residuals (y - y-hat)
lm_fit$residuals


# do these residual match what we would expect based on y - fitted.values? 
max((smoking$LUNG - lm_fit$fitted.values) - lm_fit$residuals)



```





$\\$





We can also make predictions $\hat{y}$ for new x-values





```{r}

# create a new data frame with values we want to predict
new_smoking_data <- data.frame(CIG = c(15, 25, 35))


# predicted y-hat values
(predicted_vals <- predict(lm_fit, newdata = new_smoking_data))



# replot data with these new points on it
plot(smoking$CIG, smoking$LUNG) 


# add the regression line to the plot
abline(lm_fit)


points(new_smoking_data$CIG, predicted_vals, col = "red", pch = 19)




```



$\\$





## After the break...

* Inference for linear regression
* Multiple regression 
* Data Science!









